{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T19:14:26.239278Z",
     "start_time": "2025-11-07T19:14:25.718985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "birthday = 25\n",
    "n_classes = 3 + birthday // 5 # ДР 25.07.2004 => 8\n",
    "X, y = make_multilabel_classification(n_samples=1000, n_features=10, n_classes=n_classes,\n",
    "                                     n_labels=2, random_state=42)\n",
    "y_multiclass = np.argmax(y, axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_multiclass, test_size=0.3, random_state=42)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n",
    "\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iter=1000, class_weight=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.class_weight = class_weight\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        if self.class_weight is not None:\n",
    "            sample_weights = np.where(y == 1, self.class_weight[1], self.class_weight[0])\n",
    "        else:\n",
    "            sample_weights = np.ones(n_samples)\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            predictions = sigmoid(linear_model)\n",
    "\n",
    "            error = predictions - y\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, sample_weights * error)\n",
    "            db = (1 / n_samples) * np.sum(sample_weights * error)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        return sigmoid(linear_model)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "\n",
    "class OneVsRestClassifier:\n",
    "    def __init__(self, base_classifier, class_weights=None):\n",
    "        self.base_classifier = base_classifier\n",
    "        self.classifiers = []\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.classifiers = []\n",
    "\n",
    "        for class_idx in self.classes_:\n",
    "\n",
    "            y_binary = (y == class_idx).astype(int)\n",
    "\n",
    "            if self.class_weights is not None:\n",
    "                weight_0 = self.class_weights.get('all', 1.0)\n",
    "                weight_1 = self.class_weights.get(class_idx, 1.0)\n",
    "                class_weight = {0: weight_0, 1: weight_1}\n",
    "            else:\n",
    "                class_weight = None\n",
    "\n",
    "            classifier = BinaryLogisticRegression(class_weight=class_weight)\n",
    "            classifier.fit(X, y_binary)\n",
    "            self.classifiers.append(classifier)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        probabilities = []\n",
    "        for classifier in self.classifiers:\n",
    "\n",
    "            prob = classifier.predict_proba(X)\n",
    "            probabilities.append(prob)\n",
    "\n",
    "        prob_matrix = np.column_stack(probabilities)\n",
    "        exp_probs = np.exp(prob_matrix - np.max(prob_matrix, axis=1, keepdims=True))\n",
    "        return exp_probs / np.sum(exp_probs, axis=1, keepdims=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probabilities = self.predict_proba(X)\n",
    "\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "class OneVsOneClassifier:\n",
    "    def __init__(self, base_classifier):\n",
    "        self.base_classifier = base_classifier\n",
    "        self.classifiers = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.classifiers = {}\n",
    "\n",
    "        for i, class1 in enumerate(self.classes_):\n",
    "            for class2 in self.classes_[i+1:]:\n",
    "\n",
    "                mask = (y == class1) | (y == class2)\n",
    "                X_pair = X[mask]\n",
    "                y_pair = y[mask]\n",
    "\n",
    "                y_binary = (y_pair == class1).astype(int)\n",
    "\n",
    "                classifier = BinaryLogisticRegression()\n",
    "                classifier.fit(X_pair, y_binary)\n",
    "                self.classifiers[(class1, class2)] = classifier\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        votes = np.zeros((n_samples, len(self.classes_)))\n",
    "\n",
    "        for (class1, class2), classifier in self.classifiers.items():\n",
    "            predictions = classifier.predict(X)\n",
    "\n",
    "            for i in range(n_samples):\n",
    "                if predictions[i] == 1:\n",
    "                    votes[i, class1] += 1\n",
    "                else:\n",
    "                    votes[i, class2] += 1\n",
    "\n",
    "        return np.argmax(votes, axis=1)\n",
    "\n",
    "# Сравнение методов по времени:\n",
    "start_time = time.time()\n",
    "ovr_classifier = OneVsRestClassifier(BinaryLogisticRegression(learning_rate=0.1, n_iter=1000))\n",
    "ovr_classifier.fit(X_train, y_train)\n",
    "ovr_train_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "ovo_classifier = OneVsOneClassifier(BinaryLogisticRegression(learning_rate=0.1, n_iter=1000))\n",
    "ovo_classifier.fit(X_train, y_train)\n",
    "ovo_train_time = time.time() - start_time\n",
    "\n",
    "print(f'OneVsRestClassifier: train_time = {ovr_train_time} seconds')\n",
    "print(f'OneVsOneClassifier: train_time = {ovo_train_time} seconds')\n",
    "\n",
    "print(f'Winner by time is {\"OneVsOneClassifier\" if ovr_train_time > ovo_train_time else \"OneVsRestClassifier\"} with delta = {abs(ovo_train_time-ovr_train_time)}\\n\\n')\n",
    "\n",
    "# Сравнение методов по точности\n",
    "\n",
    "y_pred = ovr_classifier.predict(X_train)\n",
    "true_values = np.sum(y_pred == y_train)\n",
    "accuracy = true_values / len(X_train)\n",
    "print(f\"OneVsRestClassifier: accuracy = {accuracy}\")\n",
    "\n",
    "y_pred_2 = ovo_classifier.predict(X_train)\n",
    "true_values_2 = np.sum(y_pred_2 == y_train)\n",
    "accuracy2 = true_values_2 / len(X_train)\n",
    "print(f\"OneVsOneClassifier: accuracy = {accuracy2}\")\n",
    "\n",
    "print(f'Winner by accuracy is {\"OneVsOneClassifier\" if accuracy2 > accuracy else \"OneVsRestClassifier\"} with delta = {abs(ovo_train_time-ovr_train_time)}\\n\\n')\n",
    "\n",
    "# Реализация f1\n",
    "\n",
    "def calculate_f1_multiclass(y_true, y_pred, average='macro'):\n",
    "    classes = np.unique(y_true)\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    TP = np.zeros(n_classes)\n",
    "    FP = np.zeros(n_classes)\n",
    "    FN = np.zeros(n_classes)\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        TP[i] = np.sum((y_true == cls) & (y_pred == cls))\n",
    "        FP[i] = np.sum((y_true != cls) & (y_pred == cls))\n",
    "        FN[i] = np.sum((y_true == cls) & (y_pred != cls))\n",
    "\n",
    "    precision = np.zeros(n_classes)\n",
    "    recall = np.zeros(n_classes)\n",
    "    f1_scores = np.zeros(n_classes)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        if TP[i] + FP[i] > 0:\n",
    "            precision[i] = TP[i] / (TP[i] + FP[i])\n",
    "        else:\n",
    "            precision[i] = 0\n",
    "\n",
    "        if TP[i] + FN[i] > 0:\n",
    "            recall[i] = TP[i] / (TP[i] + FN[i])\n",
    "        else:\n",
    "            recall[i] = 0\n",
    "\n",
    "        if precision[i] + recall[i] > 0:\n",
    "            f1_scores[i] = 2 * precision[i] * recall[i] / (precision[i] + recall[i])\n",
    "        else:\n",
    "            f1_scores[i] = 0\n",
    "\n",
    "    if average == 'macro':\n",
    "        return np.mean(f1_scores)\n",
    "    elif average == 'weighted':\n",
    "\n",
    "        class_counts = [np.sum(y_true == cls) for cls in classes]\n",
    "        weights = np.array(class_counts) / len(y_true)\n",
    "        return np.average(f1_scores, weights=weights)\n",
    "    elif average == 'micro':\n",
    "        micro_precision = np.sum(TP) / (np.sum(TP) + np.sum(FP)) if (np.sum(TP) + np.sum(FP)) > 0 else 0\n",
    "        micro_recall = np.sum(TP) / (np.sum(TP) + np.sum(FN)) if (np.sum(TP) + np.sum(FN)) > 0 else 0\n",
    "        if micro_precision + micro_recall > 0:\n",
    "            return 2 * micro_precision * micro_recall / (micro_precision + micro_recall)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "f1_ovr = calculate_f1_multiclass(y_train, ovr_classifier.predict(X_train))\n",
    "f1_ovo = calculate_f1_multiclass(y_train, ovo_classifier.predict(X_train))\n",
    "\n",
    "print(f'OneVsRestClassifier: F1-score = {f1_ovr:.4f}')\n",
    "print(f'OneVsOneClassifier: F1-score = {f1_ovo:.4f}')\n",
    "print(f'Winner by f1 is {\"OneVsOneClassifier\" if f1_ovo > f1_ovr else \"OneVsRestClassifier\"}\\n\\n')\n",
    "\n",
    "# реализация ROC-кривой, которая показывает соотношение между TP Rate и FP Rate для пороговых значений\n",
    "\n",
    "def calculate_roc_auc_multiclass(y_true, y_pred_proba, average='macro'):\n",
    "    classes = np.unique(y_true)\n",
    "    roc_auc_scores = []\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        y_true_binary = (y_true == cls).astype(int)\n",
    "        y_score = y_pred_proba[:, i]\n",
    "\n",
    "\n",
    "        sorted_indices = np.argsort(y_score)[::-1]\n",
    "        y_true_sorted = y_true_binary[sorted_indices]\n",
    "        y_score_sorted = y_score[sorted_indices]\n",
    "\n",
    "\n",
    "        thresholds = np.unique(y_score_sorted)\n",
    "        thresholds = np.append(thresholds, 1.1)\n",
    "        thresholds = np.append(-0.1, thresholds)\n",
    "        thresholds = np.sort(thresholds)[::-1]\n",
    "\n",
    "        TPR = [0.0]\n",
    "        FPR = [0.0]\n",
    "\n",
    "        total_positives = np.sum(y_true_binary)\n",
    "        total_negatives = len(y_true_binary) - total_positives\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            y_pred_binary = (y_score_sorted >= threshold).astype(int)\n",
    "\n",
    "            TP = np.sum((y_true_sorted == 1) & (y_pred_binary == 1))\n",
    "            FP = np.sum((y_true_sorted == 0) & (y_pred_binary == 1))\n",
    "\n",
    "            tpr = TP / total_positives if total_positives > 0 else 0\n",
    "            fpr = FP / total_negatives if total_negatives > 0 else 0\n",
    "\n",
    "            TPR.append(tpr)\n",
    "            FPR.append(fpr)\n",
    "\n",
    "\n",
    "        auc_score = 0\n",
    "        for j in range(1, len(FPR)):\n",
    "            auc_score += (FPR[j] - FPR[j-1]) * (TPR[j] + TPR[j-1]) / 2\n",
    "\n",
    "        roc_auc_scores.append(auc_score)\n",
    "\n",
    "    if average == 'macro':\n",
    "        return np.mean(roc_auc_scores)\n",
    "    elif average == 'weighted':\n",
    "        class_counts = [np.sum(y_true == cls) for cls in classes]\n",
    "        weights = np.array(class_counts) / len(y_true)\n",
    "        return np.average(roc_auc_scores, weights=weights)\n",
    "    else:\n",
    "        return np.mean(roc_auc_scores)\n",
    "\n",
    "y_proba_ovr = ovr_classifier.predict_proba(X_train)\n",
    "y_proba_ovo = np.zeros((len(X_train), len(np.unique(y_train))))\n",
    "\n",
    "ovo_votes = np.zeros((len(X_train), len(np.unique(y_train))))\n",
    "for (class1, class2), classifier in ovo_classifier.classifiers.items():\n",
    "    predictions = classifier.predict_proba(X_train)\n",
    "    for i in range(len(X_train)):\n",
    "        ovo_votes[i, class1] += predictions[i]\n",
    "        ovo_votes[i, class2] += (1 - predictions[i])\n",
    "\n",
    "def softmax(x, axis=1):\n",
    "    x_exp = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return x_exp / np.sum(x_exp, axis=axis, keepdims=True)\n",
    "\n",
    "y_proba_ovo = softmax(ovo_votes, axis=1)\n",
    "\n",
    "roc_auc_ovr = calculate_roc_auc_multiclass(y_train, y_proba_ovr)\n",
    "roc_auc_ovo = calculate_roc_auc_multiclass(y_train, y_proba_ovo)\n",
    "\n",
    "print(f'OneVsRestClassifier: ROC-AUC = {roc_auc_ovr:.4f}')\n",
    "print(f'OneVsOneClassifier: ROC-AUC = {roc_auc_ovo:.4f}')\n",
    "print(f'Winner by ROC-AUC is {\"OneVsOneClassifier\" if roc_auc_ovo > roc_auc_ovr else \"OneVsRestClassifier\"}\\n\\n')\n",
    "\n",
    "# реализация PR-AUC\n",
    "\n",
    "def calculate_pr_auc_multiclass(y_true, y_pred_proba, average='macro'):\n",
    "    classes = np.unique(y_true)\n",
    "    pr_auc_scores = []\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        y_true_binary = (y_true == cls).astype(int)\n",
    "        y_score = y_pred_proba[:, i]\n",
    "\n",
    "        sorted_indices = np.argsort(y_score)[::-1]\n",
    "        y_true_sorted = y_true_binary[sorted_indices]\n",
    "        y_score_sorted = y_score[sorted_indices]\n",
    "\n",
    "        thresholds = np.unique(y_score_sorted)\n",
    "        thresholds = np.append(1.1, thresholds)\n",
    "        thresholds = np.append(thresholds, -0.1)\n",
    "        thresholds = np.sort(thresholds)[::-1]\n",
    "\n",
    "        precision = [1.0]\n",
    "        recall = [0.0]\n",
    "\n",
    "        total_positives = np.sum(y_true_binary)\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            y_pred_binary = (y_score_sorted >= threshold).astype(int)\n",
    "\n",
    "            TP = np.sum((y_true_sorted == 1) & (y_pred_binary == 1))\n",
    "            FP = np.sum((y_true_sorted == 0) & (y_pred_binary == 1))\n",
    "\n",
    "            prec = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "            rec = TP / total_positives if total_positives > 0 else 0\n",
    "\n",
    "            precision.append(prec)\n",
    "            recall.append(rec)\n",
    "\n",
    "\n",
    "        auc_score = 0\n",
    "        for j in range(1, len(recall)):\n",
    "            auc_score += (recall[j] - recall[j-1]) * (precision[j] + precision[j-1]) / 2\n",
    "\n",
    "        pr_auc_scores.append(auc_score)\n",
    "\n",
    "    if average == 'macro':\n",
    "        return np.mean(pr_auc_scores)\n",
    "    elif average == 'weighted':\n",
    "        class_counts = [np.sum(y_true == cls) for cls in classes]\n",
    "        weights = np.array(class_counts) / len(y_true)\n",
    "        return np.average(pr_auc_scores, weights=weights)\n",
    "    else:\n",
    "        return np.mean(pr_auc_scores)\n",
    "\n",
    "pr_auc_ovr = calculate_pr_auc_multiclass(y_train, y_proba_ovr)\n",
    "pr_auc_ovo = calculate_pr_auc_multiclass(y_train, y_proba_ovo)\n",
    "\n",
    "print(f'OneVsRestClassifier: PR-AUC = {pr_auc_ovr:.4f}')\n",
    "print(f'OneVsOneClassifier: PR-AUC = {pr_auc_ovo:.4f}')\n",
    "print(f'Winner by PR-AUC is {\"OneVsOneClassifier\" if pr_auc_ovo > pr_auc_ovr else \"OneVsRestClassifier\"}')\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsRestClassifier: train_time = 0.11684012413024902 seconds\n",
      "OneVsOneClassifier: train_time = 0.24438881874084473 seconds\n",
      "Winner by time is OneVsRestClassifier with delta = 0.1275486946105957\n",
      "\n",
      "\n",
      "OneVsRestClassifier: accuracy = 0.67\n",
      "OneVsOneClassifier: accuracy = 0.7071428571428572\n",
      "Winner by accuracy is OneVsOneClassifier with delta = 0.1275486946105957\n",
      "\n",
      "\n",
      "OneVsRestClassifier: F1-score = 0.6323\n",
      "OneVsOneClassifier: F1-score = 0.7251\n",
      "Winner by f1 is OneVsOneClassifier\n",
      "\n",
      "\n",
      "OneVsRestClassifier: ROC-AUC = 0.9289\n",
      "OneVsOneClassifier: ROC-AUC = 0.9448\n",
      "Winner by ROC-AUC is OneVsOneClassifier\n",
      "\n",
      "\n",
      "OneVsRestClassifier: PR-AUC = 0.7510\n",
      "OneVsOneClassifier: PR-AUC = 0.7750\n",
      "Winner by PR-AUC is OneVsOneClassifier\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Задание №4: Проанализировать проблему несбалансированных классификаций.\n",
    "\n",
    "В случаях, когда один или несколько классов имеют значительно большее количество представителей, высокая accuracy не значит, что модель качественная, а в некоторых случаях модель с высоким accuracy может оказаться вовсе бесполезной.\n",
    "\n",
    "\n",
    "При использовании OneVsRestClassifier и несбалансированных данных модель быстро научится всегда предсказывать доминирующий класс, игнорируя \"Rest\", при этом вовсе не научится распозновать остальные классы. В итоге общая модель будет иметь сильное смещение в сторону мажорных классов и крайне низкую полноту для минорных классов.\n",
    "\n",
    "При использовании OneVsOneClassifier. Допустим доминирующий класс есть, его примеров 1000, кроме него есть еще 4 класса по 100, 110, 105, 95 соответсвенно. Тогда всего моделей 10 (5*4/2), 4 из которых с доминирующим классом, а остальные между собой. Тогда 6/10 моделей в хороших условиях, что говорит о том, что остальные классы тоже будут хорошо изучены. В итоге будет тоже низкая полнота для минорных классов. То есть справится лучше OneVsRestClassifier, но далеко не идеально.\n",
    "\n",
    "Данные выводы подтверждены в результате выполнения кода, в котором OneVsOneClassifier показал лучший результат, хоть и обучался дольше из-за большего кол-ва классификаторов.\n",
    "\n",
    "Кроме того, можно сделать вывод, что accuracy как метрика является бесполезной в случае несбалансированных данных, а f1, и pr_auc позволяют увидеть картину о качестве работы модели с минорными классами более ясно. В свою очередь roc_auc тоже зависит от мажорного класса, хоть и учитывает False Positive Rate, поэтому эта метрика может быть излишне завышенной при оценке модели.\n",
    "\n",
    "Для уменьшения эффекта несбалансированных данных можно использовать веса для классов, которые обратно пропорциональны их количеству. Это позволит \"выровнять\" классы между собой."
   ],
   "id": "d833375e079d6513"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
